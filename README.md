# Pipeline Batch Bovespa: ingest√£o e arquitetura de dados  - FIAP Machine Learning Tech Challenge 2

Projeto para constru√ß√£o de uma pipeline de dados completo para extrair, processar e analisar dados de a√ß√µes ou 
√≠ndices da B3

| ![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg) ![AWS S3](https://img.shields.io/badge/AWS%20S3-Data%20Lake-569A31?logo=amazons3&logoColor=white) ![AWS Glue](https://img.shields.io/badge/AWS%20Glue-ETL-6B47B2?logo=amazon-aws&logoColor=white) ![AWS Lambda](https://img.shields.io/badge/AWS%20Lambda-Scrapping/Trigger-FF9900?logo=awslambda&logoColor=white) ![Amazon Athena](https://img.shields.io/badge/Amazon%20Athena-SQL-1F73B7?logo=amazon-aws&logoColor=white) ![Amazon EventBridge](https://img.shields.io/badge/Amazon%20EventBridge-Schedule-FF4F8B?logo=amazon-aws&logoColor=white) ![MIT License](https://img.shields.io/badge/license-MIT-yellow.svg) |
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|

-----------------------------------

## Sum√°rio

- [Descri√ß√£o](#descri√ß√£o)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Arquitetura da Pipeline](#arquitetura-da-pipeline)
- [Como Utilizar](#como-utilizar)
- [Licen√ßa e Autores](#licen√ßa-e-autores)

-----------------------------------

## Descri√ß√£o

O objetivo deste projeto √© construir uma pipeline de dados batch para extrair, processar e analisar cota√ß√µes di√°rias de a√ß√µes ou √≠ndices da B3.
A ingest√£o √© feita via scraping (ou fetch de uma fonte CSV/Parquet j√° consolidada), 
os dados brutos s√£o armazenados no Amazon S3 em Parquet com parti√ß√£o di√°ria, 
um S3 Event Notification aciona uma AWS Lambda que inicia um job do AWS Glue. 
O Glue executa as transforma√ß√µes obrigat√≥rias e escreve os dados refinados (particionados por data e c√≥digo da a√ß√£o) de volta no S3. 
Por fim, os dados ficam catalogados automaticamente no Glue Data Catalog e s√£o consult√°veis via SQL no Amazon Athena.

-----------------------------------

## Tecnologias Utilizadas

### Tecnologias (Scripts / C√≥digo)

- **Python 3.11**
- **pandas**
- **pyarrow**
- **boto3**
- **requests**
- **PySpark**

### Tecnologias (AWS)

- **Amazon S3**
- **AWS Lambda**
- **AWS Glue**
- **Glue Data Catalog**
- **Amazon Athena**
- **Amazon EventBridge**
- **Amazon CloudWatch Logs**
- **AWS IAM**

-----------------------------------

## Arquitetura da Pipeline

A ingest√£o grava dados di√°rios da B3 em S3/raw (Parquet, particionado por data). 
Um evento do S3 aciona a Lambda, que inicia o AWS Glue para 
transformar os dados (agrega√ß√µes, renomea√ß√µes e m√©tricas temporais) e 
salvar em S3/refined (parti√ß√µes por data e ticker), atualizando o Glue Data Catalog. 
As consultas s√£o feitas no Amazon Athena, 
que usa o Cat√°logo para resolver o esquema e l√™ os arquivos diretamente do S3.

![Logo do Projeto](img/arch_pipeline_tech_challenge_2.png)

### üìÇ Estrutura do Reposit√≥rio

```
fiap-machine-learning-tech-challenge-2/
‚îú‚îÄ‚îÄ etl/                         
‚îÇ   ‚îî‚îÄ‚îÄ etl.py                   # Script para realizar ETL
‚îú‚îÄ‚îÄlambda/
‚îÇ   ‚îú‚îÄ‚îÄ lambda_scrapper.py       # Script contendo a lambda handler para realizar o scrapper
‚îÇ   ‚îî‚îÄ‚îÄ lambda_start_glue.py     # Script contendo a lambda handler para iniciar o glue
‚îú‚îÄ‚îÄ sanitize/                         
‚îÇ   ‚îî‚îÄ‚îÄ sanitize.py              # Limpeza b√°sica e padroniza√ß√£o de colunas num DataFrame pandas
‚îú‚îÄ‚îÄ scrapper/                         
‚îÇ   ‚îî‚îÄ‚îÄ ibov_to_s3.py            # Manipula√ß√µes para o s3
‚îî‚îÄ‚îÄ requirements.txt             # Depend√™ncias gerais do projeto
```

## Como Utilizar

O seguinte script gera o zip da Lambda do scrapper

```powershell
$ErrorActionPreference = 'Stop'

# (a) Limpar e preparar pasta de build
Remove-Item -Recurse -Force build -ErrorAction Ignore
New-Item -ItemType Directory -Path build | Out-Null

# (b) Criar venv para evitar o erro de --user e usar o pip correto
py -m venv .venv
.\.venv\Scripts\Activate.ps1

# (c) Instalar SOMENTE o que precisa dentro de build/
python -m pip install --upgrade pip
python -m pip install --no-cache-dir --target build requests==2.32.3

# (d) Copiar seu c√≥digo para build/
Copy-Item sanitize -Destination build\sanitize -Recurse
Copy-Item scrapper -Destination build\scrapper -Recurse
# (opcional) se precisar dessa pasta:
# Copy-Item outra_pasta -Destination build\outra_pasta -Recurse

# (e) Limpar lixos (opcional, s√≥ para reduzir tamanho)
Get-ChildItem build -Recurse -Include "__pycache__", "*.pyc", "tests" | Remove-Item -Recurse -Force

# (f) Gerar o ZIP  -> ZIP **do conte√∫do** da pasta build (n√£o zipar a pasta build em si!)
Compress-Archive -Path build\* -DestinationPath function.zip -Force

# (g) Desativar o venv
deactivate
```

-----------------------------------

## Licen√ßa e Autores

### üßë‚Äçüíª Desenvolvido por

- `Beatriz Rosa Carneiro Gomes - RM365967`
- `Cristine Scheibler - RM365433`
- `Guilherme Fernandes Dellatin - RM365508`
- `Iana Alexandre Neri - RM360484`
- `Jo√£o Lucas Oliveira Hilario - RM366185`

Este projeto √© apenas para fins educacionais e segue a licen√ßa MIT.